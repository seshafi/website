[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Sarah Eshafi",
    "section": "",
    "text": "Projects\n\n\n\n\nProjects\n\n\n2025\n\nForecasting Mortality Due to Extreme Temperatures ·  Group Capstone Project\n\nInteractive R Shiny dashboard to estimate mortality due to extreme temperatures across BC. Collaboration with the Office of the Provincial Health Officer as part of the Master of Data Science capstone project.\n\nBC Mental Health Services Dashboard ·  Individual Academic Project\n\nDashboard that contains all mental health and substance use services across British Columbia, developed in R Shiny. It includes an interactive map and various filters.\n\nPython Stats Package ·  Group Academic Project\n\nDeveloped a Python package called pystats_norm that performs normal distributions calculations. This package is published on PyPI and has complete documentation.\n\n\n2024\n\nHeart Disease Predictor ·  Group Academic Project\n\nMachine learning model to predict coronary heart disease given several medical tests. It includes fully reproducible analysis using containerization via docker.\n\nClimate Change Perception Analysis ·  Personal Project\n\nExploring the association between climate change perception and natural disasters, education, oil production, and press freedom."
  },
  {
    "objectID": "blog_posts/2025-01-16 ML-Interpretability/index.html",
    "href": "blog_posts/2025-01-16 ML-Interpretability/index.html",
    "title": "Unlocking the AI Black Box",
    "section": "",
    "text": "Artificial Intelligence systems, particularly those employing deep learning, are increasingly deployed in critical domains such as healthcare, finance, and autonomous driving. Despite their impressive capabilities and utility, a major obstacle to their widespread trust and adoption is the “black box” problem. This term refers to the lack of transparency in how AI models, especially neural networks, arrive at their decisions. Let’s dive deeper into the implications of this issue and explore methods to mitigate it."
  },
  {
    "objectID": "blog_posts/2025-01-16 ML-Interpretability/index.html#the-black-box-problem-in-ai",
    "href": "blog_posts/2025-01-16 ML-Interpretability/index.html#the-black-box-problem-in-ai",
    "title": "Unlocking the AI Black Box",
    "section": "The Black Box Problem in AI",
    "text": "The Black Box Problem in AI\nModern AI models, such as deep neural networks, operate with millions or billions of parameters and complex architectures. While these configurations enable high performance in tasks like image recognition or natural language processing, they also obscure the decision-making processes from human understanding. For instance, in medical imaging, why does a convolutional neural network (CNN) classify a tumor as benign? In credit risk assessment, which inputs are most important to a gradient boosting model? Without clear insights into these processes, stakeholders such as developers, regulators, and end-users face challenges in debugging, improving, and trusting these systems."
  },
  {
    "objectID": "blog_posts/2025-01-16 ML-Interpretability/index.html#the-importance-of-interpretability",
    "href": "blog_posts/2025-01-16 ML-Interpretability/index.html#the-importance-of-interpretability",
    "title": "Unlocking the AI Black Box",
    "section": "The Importance of Interpretability",
    "text": "The Importance of Interpretability\nIn certain low-risk settings, it may not matter why a model makes certain decisions. For example, a movie prediction algorithm does not have severe consequences for being incorrect. However, there are several circumstances where interpretability is important.\n\nSafety & Reliability\nIn high-stakes applications, unexplained AI behavior can lead to catastrophic failures. In autonomous vehicles, a model’s opacity on why it misclassified an object on the road could result in fatal accidents. Clear explanations of model failures help data scientists identify and address underlying issues.\n\n\nBias & Discrimination\nAI systems trained on biased datasets can propagate and amplify these biases. For example, a hiring algorithm trained on historical data may discriminate based on gender or ethnicity. Interpretability allows developers to identify and address these issues.\n\n\nRegulatory Compliance\nRegulations like the EU’s AI Act require transparency for high-risk AI systems (EUAIAct.com 2025). Non-compliance could result in significant legal and financial penalties. For example, banks are required to explain why certain loan requests are denied.\n\n\nModel Improvement & Understanding\nDevelopers need to understand model behavior to diagnose errors, optimize performance, and ensure generalization to unseen data. Understanding a model’s limitations enables developers to address edge cases effectively. Furthermore, a robust understanding of model predictions can help us understand more about the world. This is particularly important in a scientific context."
  },
  {
    "objectID": "blog_posts/2025-01-16 ML-Interpretability/index.html#approaches-to-enhancing-interpretability",
    "href": "blog_posts/2025-01-16 ML-Interpretability/index.html#approaches-to-enhancing-interpretability",
    "title": "Unlocking the AI Black Box",
    "section": "Approaches to Enhancing Interpretability",
    "text": "Approaches to Enhancing Interpretability\nSeveral strategies exist to tackle the black box problem, and they can be broadly categorized into: (1) using interpretable models or (2) leveraging post-hoc techniques. There are two types of interpretability we will consider: local (i.e. why did my model make this specific prediction?) and global (which features are most important to my model?). This is an active area of research, and this blog post will scratch the surface of the most popular, traditional methods that you need to know.\n\nInterpretable Models\nIn cases where transparency is highly important, some researchers opt for inherently interpretable models. Linear regression models provide both local and global interpretations, using coefficient weights. Decision trees are another example of an interpretable model, given the tree split thresholds and features. There is some research that these models do not sacrifice prediction performance in certain domains (Rudin and Radin 2019). However, this is not always the case and black-box models are often better suited to make predictions. In this case, post-hoc approaches are often used to explain model performance.\n\n\nPost-Hoc Explanation Methods\nPost-hoc analysis explain the behavior of a machine learning model after it has been trained. These methods aim to provide insights into how a model makes predictions, even if the model itself is complex and non-interpretable. Let’s explore some post-hoc approaches.\n\nSurrogate Models\nSurrogate models are simplified, interpretable models trained to mimic the behavior of a complex black-box model. The primary goal is to approximate the decision-making process of the original model in a way that humans can easily understand.\nHow It Works: Generate predictions from the black-box model for a given dataset. Then, train a simpler, interpretable model (e.g., a decision tree or linear regression) using the original input data and the predictions from the black-box model as the target.\nThis can identify global feature importances in any black-box model. However, the surrogate models are inherently less accurate and often oversimplify the problem. They may not fully capture the nuances of the black-box model.\n\n\nLIME (Local Interpretable Model-Agnostic Explanations)\nLIME is a popular technique for local interpretability for any machine learning model. It applies the same principle as surrogate models but zoomed in to a narrow data range.\nHow it works: Take a prediction you want to explain and generate a synthetic dataset by perturbing the instance’s features (e.g., adding noise or slightly modifying values). Obtain predictions for the perturbed samples from the black-box model and train a simple model (e.g., linear regression) using the perturbed samples as input and their corresponding predictions as the target. Finally, analyze the weights of the interpretable model to understand the importance of features for the specific prediction.\nThis approach is commonly used because it can be applied to any model flavour and it provides clear explanations for individual predictions. However, it can be computationally expensive for large datasets or complex models.\n\n\nSHAP (SHapley Additive Explanations)\nSHAP is another commonly used post-hoc analysis approach that can be applied to any model. It is rooted in cooperative game theory.\nHow It Works: Treat the prediction task as a cooperative game where features are players. Calculate the marginal contribution of each feature by comparing the model’s prediction with and without the feature. Aggregate these contributions across all possible combinations of features to compute Shapley values.\nSHAP provides both local and global interpretations. It offers a theoretically sound and consistent feature importance values. However, it is computationally intensive, so approximations are usually required for complex models.\n\n\n\n\n\n\n\n\n\n\n\nSHAP values for a specific prediction. The features are along the y-axis and the prediction is on the x-axis. The figure should be read from bottom to top. (Lundberg 2018).\n\n\n\n\n\nPartial Dependence Plots (PDP)\nPartial dependence plots illustrate the marginal effect of one or two features on the average prediction of a model. They are particularly useful for understanding how specific features influence the model’s output globally.\nHow It Works: With the value of the target feature(s) fixed at different levels, average the model’s predictions over all instances, keeping other features constant. Plot the results to visualize the feature’s impact (see figure below).\n\n\n\n\n\n\n\n\nPartial Dependence Plots for 3 separate features (Molnar 2024).\n\n\n\nPDPs are intuitive and easy to interpret. They are also useful for identifying non-linear relationships between features and predictions. On the other hand, this approach assumes feature independence, which may lead to misleading results in datasets with correlated features.\n\n\nIndividual Conditional Expectations (ICE)\nICE plots extend PDPs by showing the effect of a feature for individual data points rather than averaging across all data. Each line in an ICE plot represents the prediction trajectory for a single instance.\nHow It Works: Similar to PDPs, vary the target feature’s value across its range. Instead of averaging predictions, plot the trajectory for each individual instance (see figure below).\n\n\n\n\n\n\n\n\n\n\n\nICE Plots for the feature Age and prediction Cancer Probability (Molnar 2024).\n\n\n\nICE plots are used to uncover granular insights into feature effects at the instance level. They highlight heterogeneous effects that PDPs might obscure, but they can become easily cluttered with too many data points.\n\n\nComparison\n\n\n\n\n\n\n\n\n\n\nMethod\nStrengths\nLimitations\nPython Libraries\n\n\n\n\n\nSurrogate Models\nSimple and interpretable; model-agnostic\nMay oversimplify black-box model\nScikit-learn\n\n\n\nLIME\nLocalized explanations; flexible\nComputationally expensive; only local insights\nLIME\n\n\n\nSHAP\nTheoretically sound; global and local explanations\nComputationally intensive\nSHAP\n\n\n\nPDP\nIntuitive; highlights feature relationships\nAssumes feature independence\nScikit-learn, PDPbox\n\n\n\nICE\nGranular, instance-level insights\nCan be cluttered\nScikit-learn, PDPbox"
  },
  {
    "objectID": "blog_posts/2025-01-16 ML-Interpretability/index.html#the-path-forward",
    "href": "blog_posts/2025-01-16 ML-Interpretability/index.html#the-path-forward",
    "title": "Unlocking the AI Black Box",
    "section": "The Path Forward",
    "text": "The Path Forward\nAs artificial intelligence becomes more integral to critical applications, the need for interpretable models grows more urgent. From healthcare to finance to autonomous systems, the ability to explain how AI models arrive at their decisions is essential to ensuring safety, fairness, and compliance with regulatory standards.\nInterpretability is not just a desirable feature but a necessity for responsible AI development. Transparent models enable stakeholders to enhance safety and reliability, mitigate bias and promote fairness, ensure regulatory compliance, and improve model performance.\nWhile simpler, inherently interpretable models provide clarity, they are often unsuitable for the complexity of modern tasks. In these cases, advanced interpretability techniques allow us to gain insights into black-box models without sacrificing performance.\nThe path forward requires a concerted effort to balance AI’s growing complexity with its usability and trustworthiness. By embedding interpretability as a foundational principle, we can build AI systems that are not only powerful but also aligned with human values, enabling their safe integration into society.\nIf you want to learn more about the technical details of ML interpretability approaches included in this post, I strongly recommend Christoph Molnar’s book “Interpretable Machine Learning”."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Sarah’s Blog",
    "section": "",
    "text": "Unlocking the AI Black Box\n\n\nThe Case and Approach for Interpretable AI\n\n\n\ncomparison\n\n\n\n\n\n\n\n\n\nJan 18, 2025\n\n\nSarah Eshafi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sarah Eshafi",
    "section": "",
    "text": "Welcome to my website.\nA bit about me: I am a recent UBC Master of Data Science graduate (June 2025). I previously completed an Honours BSc in Integrated Science with a major in Mathematics and Statistics from McMaster University.\nPrior to my Masters, I worked in consulting at Isaac Operations, where I helped clients across various industries optimize their operations. Later, I transitioned into a senior analyst role for the same company, developing tools and dashboards to support data-driven decision-making for our clients and internally.\nMy passion lies in applying analytics and machine learning to drive real business outcomes. I’m motivated by creating meaningful, positive impact with my work and solving complex problems, which drives me to continue learning.\nI have experience developing machine learning models, dashboards, and analytics solutions across healthcare and manufacturing sectors. I’ve built ML forecasting models for the BC Office of the Provincial Health Officer and led analytics projects that delivered measurable impact. My main technical skills include Python, R, SQL, Power BI, Excel, data visualization, and end-to-end ML pipelines.\nIn my free time, I enjoy camping, reading, cooking, and rotating through a number of crafty hobbies (currently getting back into cross-stitch!).\nOn this site you can find my blog and some notable projects."
  },
  {
    "objectID": "index.html#hi-im-sarah",
    "href": "index.html#hi-im-sarah",
    "title": "Sarah Eshafi",
    "section": "",
    "text": "Welcome to my website.\nA bit about me: I am a recent UBC Master of Data Science graduate (June 2025). I previously completed an Honours BSc in Integrated Science with a major in Mathematics and Statistics from McMaster University.\nPrior to my Masters, I worked in consulting at Isaac Operations, where I helped clients across various industries optimize their operations. Later, I transitioned into a senior analyst role for the same company, developing tools and dashboards to support data-driven decision-making for our clients and internally.\nMy passion lies in applying analytics and machine learning to drive real business outcomes. I’m motivated by creating meaningful, positive impact with my work and solving complex problems, which drives me to continue learning.\nI have experience developing machine learning models, dashboards, and analytics solutions across healthcare and manufacturing sectors. I’ve built ML forecasting models for the BC Office of the Provincial Health Officer and led analytics projects that delivered measurable impact. My main technical skills include Python, R, SQL, Power BI, Excel, data visualization, and end-to-end ML pipelines.\nIn my free time, I enjoy camping, reading, cooking, and rotating through a number of crafty hobbies (currently getting back into cross-stitch!).\nOn this site you can find my blog and some notable projects."
  }
]