<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sarah Eshafi">
<meta name="dcterms.date" content="2025-01-18">

<title>Unlocking the AI Black Box – Sarah Eshafi</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Sarah Eshafi</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-black-box-problem-in-ai" id="toc-the-black-box-problem-in-ai" class="nav-link active" data-scroll-target="#the-black-box-problem-in-ai">The Black Box Problem in AI</a></li>
  <li><a href="#the-importance-of-interpretability" id="toc-the-importance-of-interpretability" class="nav-link" data-scroll-target="#the-importance-of-interpretability">The Importance of Interpretability</a>
  <ul class="collapse">
  <li><a href="#safety-reliability" id="toc-safety-reliability" class="nav-link" data-scroll-target="#safety-reliability">Safety &amp; Reliability</a></li>
  <li><a href="#bias-discrimination" id="toc-bias-discrimination" class="nav-link" data-scroll-target="#bias-discrimination">Bias &amp; Discrimination</a></li>
  <li><a href="#regulatory-compliance" id="toc-regulatory-compliance" class="nav-link" data-scroll-target="#regulatory-compliance">Regulatory Compliance</a></li>
  <li><a href="#model-improvement-understanding" id="toc-model-improvement-understanding" class="nav-link" data-scroll-target="#model-improvement-understanding">Model Improvement &amp; Understanding</a></li>
  </ul></li>
  <li><a href="#approaches-to-enhancing-interpretability" id="toc-approaches-to-enhancing-interpretability" class="nav-link" data-scroll-target="#approaches-to-enhancing-interpretability">Approaches to Enhancing Interpretability</a>
  <ul class="collapse">
  <li><a href="#interpretable-models" id="toc-interpretable-models" class="nav-link" data-scroll-target="#interpretable-models">Interpretable Models</a></li>
  <li><a href="#post-hoc-explanation-methods" id="toc-post-hoc-explanation-methods" class="nav-link" data-scroll-target="#post-hoc-explanation-methods">Post-Hoc Explanation Methods</a></li>
  </ul></li>
  <li><a href="#the-path-forward" id="toc-the-path-forward" class="nav-link" data-scroll-target="#the-path-forward">The Path Forward</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Unlocking the AI Black Box</h1>
<p class="subtitle lead">The Case and Approach for Interpretable AI</p>
  <div class="quarto-categories">
    <div class="quarto-category">comparison</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Sarah Eshafi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 18, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>Artificial Intelligence systems, particularly those employing deep learning, are increasingly deployed in critical domains such as healthcare, finance, and autonomous driving. Despite their impressive capabilities and utility, a major obstacle to their widespread trust and adoption is the “black box” problem. This term refers to the lack of transparency in how AI models, especially neural networks, arrive at their decisions. Let’s dive deeper into the implications of this issue and explore methods to mitigate it.</p>
<section id="the-black-box-problem-in-ai" class="level2">
<h2 class="anchored" data-anchor-id="the-black-box-problem-in-ai">The Black Box Problem in AI</h2>
<p>Modern AI models, such as deep neural networks, operate with millions or billions of parameters and complex architectures. While these configurations enable high performance in tasks like image recognition or natural language processing, they also obscure the decision-making processes from human understanding. For instance, in medical imaging, why does a convolutional neural network (CNN) classify a tumor as benign? In credit risk assessment, which inputs are most important to a gradient boosting model? Without clear insights into these processes, stakeholders such as developers, regulators, and end-users face challenges in debugging, improving, and trusting these systems.</p>
</section>
<section id="the-importance-of-interpretability" class="level2">
<h2 class="anchored" data-anchor-id="the-importance-of-interpretability">The Importance of Interpretability</h2>
<p>In certain low-risk settings, it may not matter why a model makes certain decisions. For example, a movie prediction algorithm does not have severe consequences for being incorrect. However, there are several circumstances where interpretability is important.</p>
<section id="safety-reliability" class="level3">
<h3 class="anchored" data-anchor-id="safety-reliability">Safety &amp; Reliability</h3>
<p>In high-stakes applications, unexplained AI behavior can lead to catastrophic failures. In autonomous vehicles, a model’s opacity on why it misclassified an object on the road could result in fatal accidents. Clear explanations of model failures help data scientists identify and address underlying issues.</p>
</section>
<section id="bias-discrimination" class="level3">
<h3 class="anchored" data-anchor-id="bias-discrimination">Bias &amp; Discrimination</h3>
<p>AI systems trained on biased datasets can propagate and amplify these biases. For example, a hiring algorithm trained on historical data may discriminate based on gender or ethnicity. Interpretability allows developers to identify and address these issues.</p>
</section>
<section id="regulatory-compliance" class="level3">
<h3 class="anchored" data-anchor-id="regulatory-compliance">Regulatory Compliance</h3>
<p>Regulations like the EU’s AI Act require transparency for high-risk AI systems <span class="citation" data-cites="EU_reg">(<a href="#ref-EU_reg" role="doc-biblioref">EUAIAct.com 2025</a>)</span>. Non-compliance could result in significant legal and financial penalties. For example, banks are required to explain why certain loan requests are denied.</p>
</section>
<section id="model-improvement-understanding" class="level3">
<h3 class="anchored" data-anchor-id="model-improvement-understanding">Model Improvement &amp; Understanding</h3>
<p>Developers need to understand model behavior to diagnose errors, optimize performance, and ensure generalization to unseen data. Understanding a model’s limitations enables developers to address edge cases effectively. Furthermore, a robust understanding of model predictions can help us understand more about the world. This is particularly important in a scientific context.</p>
</section>
</section>
<section id="approaches-to-enhancing-interpretability" class="level2">
<h2 class="anchored" data-anchor-id="approaches-to-enhancing-interpretability">Approaches to Enhancing Interpretability</h2>
<p>Several strategies exist to tackle the black box problem, and they can be broadly categorized into: (1) using interpretable models or (2) leveraging post-hoc techniques. There are two types of interpretability we will consider: local (i.e.&nbsp;why did my model make this specific prediction?) and global (which features are most important to my model?). This is an active area of research, and this blog post will scratch the surface of the most popular, traditional methods that you need to know.</p>
<section id="interpretable-models" class="level3">
<h3 class="anchored" data-anchor-id="interpretable-models">Interpretable Models</h3>
<p>In cases where transparency is highly important, some researchers opt for inherently interpretable models. Linear regression models provide both local and global interpretations, using coefficient weights. Decision trees are another example of an interpretable model, given the tree split thresholds and features. There is some research that these models do not sacrifice prediction performance in certain domains <span class="citation" data-cites="int_models">(<a href="#ref-int_models" role="doc-biblioref">Rudin and Radin 2019</a>)</span>. However, this is not always the case and black-box models are often better suited to make predictions. In this case, post-hoc approaches are often used to explain model performance.</p>
</section>
<section id="post-hoc-explanation-methods" class="level3">
<h3 class="anchored" data-anchor-id="post-hoc-explanation-methods">Post-Hoc Explanation Methods</h3>
<p>Post-hoc analysis explain the behavior of a machine learning model after it has been trained. These methods aim to provide insights into how a model makes predictions, even if the model itself is complex and non-interpretable. Let’s explore some post-hoc approaches.</p>
<section id="surrogate-models" class="level4">
<h4 class="anchored" data-anchor-id="surrogate-models">Surrogate Models</h4>
<p>Surrogate models are simplified, interpretable models trained to mimic the behavior of a complex black-box model. The primary goal is to approximate the decision-making process of the original model in a way that humans can easily understand.</p>
<p>How It Works: Generate predictions from the black-box model for a given dataset. Then, train a simpler, interpretable model (e.g., a decision tree or linear regression) using the original input data and the predictions from the black-box model as the target.</p>
<p>This can identify global feature importances in any black-box model. However, the surrogate models are inherently less accurate and often oversimplify the problem. They may not fully capture the nuances of the black-box model.</p>
</section>
<section id="lime-local-interpretable-model-agnostic-explanations" class="level4">
<h4 class="anchored" data-anchor-id="lime-local-interpretable-model-agnostic-explanations">LIME (Local Interpretable Model-Agnostic Explanations)</h4>
<p>LIME is a popular technique for local interpretability for any machine learning model. It applies the same principle as surrogate models but zoomed in to a narrow data range.</p>
<p>How it works: Take a prediction you want to explain and generate a synthetic dataset by perturbing the instance’s features (e.g., adding noise or slightly modifying values). Obtain predictions for the perturbed samples from the black-box model and train a simple model (e.g., linear regression) using the perturbed samples as input and their corresponding predictions as the target. Finally, analyze the weights of the interpretable model to understand the importance of features for the specific prediction.</p>
<p>This approach is commonly used because it can be applied to any model flavour and it provides clear explanations for individual predictions. However, it can be computationally expensive for large datasets or complex models.</p>
</section>
<section id="shap-shapley-additive-explanations" class="level4">
<h4 class="anchored" data-anchor-id="shap-shapley-additive-explanations">SHAP (SHapley Additive Explanations)</h4>
<p>SHAP is another commonly used post-hoc analysis approach that can be applied to any model. It is rooted in cooperative game theory.</p>
<p>How It Works: Treat the prediction task as a cooperative game where features are players. Calculate the marginal contribution of each feature by comparing the model’s prediction with and without the feature. Aggregate these contributions across all possible combinations of features to compute Shapley values.</p>
<p>SHAP provides both local and global interpretations. It offers a theoretically sound and consistent feature importance values. However, it is computationally intensive, so approximations are usually required for complex models.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 100%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><img src="SHAP.png" class="img-fluid"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><em>SHAP values for a specific prediction. The features are along the y-axis and the prediction is on the x-axis. The figure should be read from bottom to top. <span class="citation" data-cites="SHAP_docs">(<a href="#ref-SHAP_docs" role="doc-biblioref">Lundberg 2018</a>)</span>.</em></td>
</tr>
</tbody>
</table>
</section>
<section id="partial-dependence-plots-pdp" class="level4">
<h4 class="anchored" data-anchor-id="partial-dependence-plots-pdp">Partial Dependence Plots (PDP)</h4>
<p>Partial dependence plots illustrate the marginal effect of one or two features on the average prediction of a model. They are particularly useful for understanding how specific features influence the model’s output globally.</p>
<p>How It Works: With the value of the target feature(s) fixed at different levels, average the model’s predictions over all instances, keeping other features constant. Plot the results to visualize the feature’s impact (see figure below).</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;"><img src="PDP.png" class="img-fluid"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><em>Partial Dependence Plots for 3 separate features <span class="citation" data-cites="ml_book">(<a href="#ref-ml_book" role="doc-biblioref">Molnar 2024</a>)</span>.</em></td>
</tr>
</tbody>
</table>
<p>PDPs are intuitive and easy to interpret. They are also useful for identifying non-linear relationships between features and predictions. On the other hand, this approach assumes feature independence, which may lead to misleading results in datasets with correlated features.</p>
</section>
<section id="individual-conditional-expectations-ice" class="level4">
<h4 class="anchored" data-anchor-id="individual-conditional-expectations-ice">Individual Conditional Expectations (ICE)</h4>
<p>ICE plots extend PDPs by showing the effect of a feature for individual data points rather than averaging across all data. Each line in an ICE plot represents the prediction trajectory for a single instance.</p>
<p>How It Works: Similar to PDPs, vary the target feature’s value across its range. Instead of averaging predictions, plot the trajectory for each individual instance (see figure below).</p>
<table class="caption-top table">
<colgroup>
<col style="width: 100%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><img src="ICE.png" class="img-fluid"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><em>ICE Plots for the feature Age and prediction Cancer Probability <span class="citation" data-cites="ml_book">(<a href="#ref-ml_book" role="doc-biblioref">Molnar 2024</a>)</span>.</em></td>
</tr>
</tbody>
</table>
<p>ICE plots are used to uncover granular insights into feature effects at the instance level. They highlight heterogeneous effects that PDPs might obscure, but they can become easily cluttered with too many data points.</p>
</section>
<section id="comparison" class="level4">
<h4 class="anchored" data-anchor-id="comparison">Comparison</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Strengths</th>
<th>Limitations</th>
<th>Python Libraries</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Surrogate Models</td>
<td>Simple and interpretable; model-agnostic</td>
<td>May oversimplify black-box model</td>
<td>Scikit-learn</td>
<td></td>
</tr>
<tr class="even">
<td>LIME</td>
<td>Localized explanations; flexible</td>
<td>Computationally expensive; only local insights</td>
<td>LIME</td>
<td></td>
</tr>
<tr class="odd">
<td>SHAP</td>
<td>Theoretically sound; global and local explanations</td>
<td>Computationally intensive</td>
<td>SHAP</td>
<td></td>
</tr>
<tr class="even">
<td>PDP</td>
<td>Intuitive; highlights feature relationships</td>
<td>Assumes feature independence</td>
<td>Scikit-learn, PDPbox</td>
<td></td>
</tr>
<tr class="odd">
<td>ICE</td>
<td>Granular, instance-level insights</td>
<td>Can be cluttered</td>
<td>Scikit-learn, PDPbox</td>
<td></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="the-path-forward" class="level2">
<h2 class="anchored" data-anchor-id="the-path-forward">The Path Forward</h2>
<p>As artificial intelligence becomes more integral to critical applications, the need for interpretable models grows more urgent. From healthcare to finance to autonomous systems, the ability to explain how AI models arrive at their decisions is essential to ensuring safety, fairness, and compliance with regulatory standards.</p>
<p>Interpretability is not just a desirable feature but a necessity for responsible AI development. Transparent models enable stakeholders to enhance safety and reliability, mitigate bias and promote fairness, ensure regulatory compliance, and improve model performance.</p>
<p>While simpler, inherently interpretable models provide clarity, they are often unsuitable for the complexity of modern tasks. In these cases, advanced interpretability techniques allow us to gain insights into black-box models without sacrificing performance.</p>
<p>The path forward requires a concerted effort to balance AI’s growing complexity with its usability and trustworthiness. By embedding interpretability as a foundational principle, we can build AI systems that are not only powerful but also aligned with human values, enabling their safe integration into society.</p>
<p>If you want to learn more about the technical details of ML interpretability approaches included in this post, I strongly recommend <a href="https://christophm.github.io/interpretable-ml-book/index.html">Christoph Molnar’s book “Interpretable Machine Learning”</a>.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-EU_reg" class="csl-entry" role="listitem">
EUAIAct.com. 2025. <span>“EU AI ACT Transparency Obligations.”</span> <a href="https://www.euaiact.com/key-issue/5">https://www.euaiact.com/key-issue/5</a>.
</div>
<div id="ref-SHAP_docs" class="csl-entry" role="listitem">
Lundberg, Scott. 2018. <span>“An Introduction to Explainable AI with Shapley Values.”</span> <a href="https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html">https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html</a>.
</div>
<div id="ref-ml_book" class="csl-entry" role="listitem">
Molnar, Christoph. 2024. <em>Interpretable Machine Learning</em>. <a href="https://christophm.github.io/interpretable-ml-book/index.html">https://christophm.github.io/interpretable-ml-book/index.html</a>.
</div>
<div id="ref-int_models" class="csl-entry" role="listitem">
Rudin, Cynthia, and Joanna Radin. 2019. <span>“Why Are We Using Black Box Models in AI When We Don’t Need to? A Lesson from an Explainable AI Competition.”</span> <a href="https://hdsr.mitpress.mit.edu/pub/f9kuryi8/release/8">https://hdsr.mitpress.mit.edu/pub/f9kuryi8/release/8</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>